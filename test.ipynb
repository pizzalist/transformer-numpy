{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "pe = torch.zeros(3, 4)\n",
    "print(pe)\n",
    "print(pe[:, 0::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3,5)\n",
    "a.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.],\n",
       "        [5.],\n",
       "        [6.],\n",
       "        [7.],\n",
       "        [8.],\n",
       "        [9.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 10, dtype = torch.float).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.210340371976184"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "-math.log(10000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len, d_model = 5, 6\n",
    "pe = torch.zeros(seq_len, d_model) \n",
    "\n",
    "# Creating a tensor representing positions (0 to seq_len - 1)\n",
    "position = torch.arange(0, seq_len, dtype = torch.float).unsqueeze(1) # Transforming 'position' into a 2D tensor['seq_len, 1']\n",
    "\n",
    "# Creating the division term for the positional encoding formula\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "# Apply sine to even indices in pe\n",
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "# Apply cosine to odd indices in pe\n",
    "pe[:, 1::2] = torch.cos(position * div_term)\n",
    "pe = pe.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, seq_len, d_model =2, 5, 6\n",
    "pe = torch.zeros(batch_size, seq_len, d_model) \n",
    "pe.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 512])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from embedding.pt_position import PositionalEncoding\n",
    "\n",
    "pos_encoding = PositionalEncoding(d_model=512, seq_len=10, dropout=0.1)\n",
    "print(pos_encoding.pe.shape)\n",
    "x = torch.rand(5, 10, 512)\n",
    "output = pos_encoding(x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, 512)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "d_model = 512\n",
    "seq_len = 10\n",
    "\n",
    "position = np.arange(seq_len, dtype=float)[:, np.newaxis] \n",
    "div_term = np.exp(np.arange(0, d_model, 2, dtype=float)) #* -(np.log(10000.0) / d_model))\n",
    "pe = np.zeros((seq_len, d_model))\n",
    "pe[:, 0::2] = np.sin(position * div_term)\n",
    "# Apply cosine to odd indices in pe\n",
    "pe[:, 1::2] = np.cos(position * div_term)\n",
    "pe = np.expand_dims(pe, axis=0)\n",
    "pe.shape\n",
    "# x = np.random.rand(5, 10, 512)\n",
    "\n",
    "pe[:, :x.shape[1]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 2 3]\n",
      "  [4 5 6]]]\n",
      "Shape of result1: (1, 2, 3)\n",
      "[[[1 2 3]\n",
      "  [4 5 6]]]\n",
      "Shape of result2: (1, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# pe 배열: (1, 4, 3) 형태의 3차원 배열\n",
    "pe = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]])\n",
    "\n",
    "# x 배열: (2, 2, 3) 형태의 3차원 배열\n",
    "x = np.random.rand(2, 2, 3)\n",
    "\n",
    "# pe[:, :x.shape[1]] 사용\n",
    "result1 = pe[:, :x.shape[1]]\n",
    "print(result1)\n",
    "print(\"Shape of result1:\", result1.shape)\n",
    "\n",
    "# pe[:, :x.shape[1], :] 사용\n",
    "result2 = pe[:, :x.shape[1], :]\n",
    "print(result2)\n",
    "print(\"Shape of result2:\", result2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.8974e-02, -3.7333e-01, -1.2893e+00,  ..., -1.0914e+00,\n",
       "          -1.4434e+00, -4.5408e-01],\n",
       "         [-9.6912e-01,  1.4086e+00, -3.6165e-01,  ...,  7.4841e-02,\n",
       "          -4.1845e-01,  4.6434e-04],\n",
       "         [ 1.8011e+00,  1.6231e+00,  9.6974e-01,  ...,  2.4877e-01,\n",
       "           8.8367e-01,  1.0459e-01],\n",
       "         ...,\n",
       "         [ 1.5495e+00, -2.4305e-01,  1.1839e+00,  ..., -7.5035e-01,\n",
       "          -1.0053e+00,  6.2943e-01],\n",
       "         [ 5.8528e-01,  1.1627e-01, -4.7579e-01,  ..., -7.7362e-01,\n",
       "           2.3335e-01, -3.1856e-02],\n",
       "         [ 1.0478e+00,  1.6069e+00, -1.4900e+00,  ..., -3.6156e-01,\n",
       "          -4.6621e-01,  1.1313e+00]],\n",
       "\n",
       "        [[ 1.4661e+00, -9.2386e-02,  1.3521e+00,  ...,  3.2525e-01,\n",
       "          -6.6381e-01, -9.4885e-01],\n",
       "         [ 1.4223e+00, -1.4152e+00, -1.3470e+00,  ...,  4.6218e-01,\n",
       "           1.2839e+00, -1.2251e+00],\n",
       "         [ 4.8338e-01,  1.5696e+00, -7.2131e-01,  ..., -6.4744e-01,\n",
       "           8.2422e-03, -6.0463e-01],\n",
       "         ...,\n",
       "         [-1.6825e+00,  1.2184e+00,  8.4737e-02,  ...,  1.1206e+00,\n",
       "           1.5251e+00, -8.6183e-01],\n",
       "         [ 1.2871e+00, -1.1304e+00, -1.3292e+00,  ..., -4.1422e-01,\n",
       "          -1.4452e+00,  1.7855e-02],\n",
       "         [-2.7602e-01,  1.6851e+00, -3.3825e-01,  ..., -6.3957e-01,\n",
       "           1.0654e+00, -1.4320e+00]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "eps = 10**-6\n",
    "x = torch.tensor(np.random.rand(2, 10, 512), dtype=torch.float32)\n",
    "alpha = nn.Parameter(torch.ones(1)) # One-dimensional tensor that will be used to scale the input data\n",
    "\n",
    "bias = nn.Parameter(torch.zeros(1)) # One-dimensional tenso that will be added to the input data\n",
    "alpha, bias\n",
    "\n",
    "mean = x.mean(dim = -1, keepdim = True) # Computing the mean of the input data. Keeping the number of dimensions unchanged\n",
    "std = x.std(dim = -1, keepdim = True) # Computing the standard deviation of the input data. Keeping the number of dimensions unchanged\n",
    "        \n",
    "alpha * (x-mean) / (std + eps) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.406948  ,  1.25346707, -0.42135117, ..., -0.97828444,\n",
       "          1.6543173 , -0.46640261],\n",
       "        [ 0.59150044,  0.99039293, -0.40004363, ...,  0.21679665,\n",
       "         -0.37536094,  0.17018866],\n",
       "        [-0.90687934,  0.31464981, -0.90301743, ...,  0.65993653,\n",
       "         -1.31050138,  0.54569101],\n",
       "        ...,\n",
       "        [-1.21250908, -0.27937219, -0.00859462, ..., -0.19181777,\n",
       "          1.17208064,  1.01407876],\n",
       "        [-0.01594736, -0.2376258 ,  1.3790151 , ...,  1.01714823,\n",
       "          0.70535331,  0.12603634],\n",
       "        [-0.29881791, -1.56283988, -0.63784847, ...,  1.31632397,\n",
       "         -1.31980812,  1.00559515]],\n",
       "\n",
       "       [[-1.49989245,  1.02475926, -0.1921105 , ..., -0.04745001,\n",
       "         -1.34603429, -0.78242785],\n",
       "        [-0.70973579,  1.41222795, -0.24558219, ...,  0.42040351,\n",
       "          0.5467505 , -0.28627766],\n",
       "        [ 0.91151198, -0.03843883, -0.51292036, ..., -1.30789123,\n",
       "         -0.96578559,  0.10643825],\n",
       "        ...,\n",
       "        [ 1.590901  , -0.60996341, -0.20250717, ...,  0.26296964,\n",
       "          1.7038506 , -0.81559752],\n",
       "        [ 0.45552191,  1.51030739, -1.27669041, ..., -0.14840544,\n",
       "          0.62068237, -0.89572402],\n",
       "        [-1.55494965,  0.17569387,  0.45620587, ..., -1.56184161,\n",
       "          1.46038497,  0.94949201]]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "eps = 10**-6\n",
    "alpha = np.ones((1,)) \n",
    "bias = np.zeros((1,))\n",
    "x = np.random.rand(2, 10, 512)\n",
    "mean = x.mean(axis = -1, keepdims=True) # Computing the mean of the input data. Keeping the number of dimensions unchanged\n",
    "std = x.std(axis = -1, keepdims=True) # Computing the standard deviation of the input data. Keeping the number of dimensions unchanged\n",
    "        \n",
    "alpha * (x-mean) / (std + eps) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed forward test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 10, 512)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x, train_flag=True):\n",
    "        # 학습 시\n",
    "        if train_flag:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_rate\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_rate)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "\n",
    "class ReLU:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, x, grad_output):\n",
    "        relu_grad = x > 0\n",
    "        return grad_output * relu_grad\n",
    "\n",
    "class FeedForwardBlock:\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        # forward만 구현, random한 값으로 임의 init\n",
    "        self.W1 = np.random.randn(d_model, d_ff) \n",
    "        self.b1 = np.random.randn(d_ff)\n",
    "        self.W2 = np.random.randn(d_ff, d_model)\n",
    "        self.b2 = np.random.randn(d_model)\n",
    "        \n",
    "        self.actiavation = ReLU()\n",
    "        self.dropout = Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # (Batch, seq_len, d_model) --> (batch, seq_len, d_ff)\n",
    "        linear_1 = self.dropout.forward(self.actiavation.forward(np.dot(x, self.W1) + self.b1))\n",
    "        \n",
    "        # (batch, seq_len, d_ff) -->(batch, seq_len, d_model)\n",
    "        linear_2 = np.dot(linear_1, self.W2) + self.b2\n",
    "        return linear_2\n",
    "\n",
    "FeedForwardBlock(512, 2048, 0.1).forward(np.random.rand(2, 10, 512)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[False  True]\n",
      "  [False  True]]]\n",
      "[[[-1.00000000e+09  2.83581647e-01]\n",
      "  [-1.00000000e+09  1.31930548e-01]]]\n"
     ]
    }
   ],
   "source": [
    "# qyery, key, value shape: (batch_size, seq_len, d_k)\n",
    "# mask shape: (batch_size, seq_len, seq_len)\n",
    "\n",
    "query, key, value = np.random.rand(1, 2, 5), np.random.rand(1, 2, 5), np.random.rand(1, 2, 5)\n",
    "mask = np.random.rand(1, 2, 2) > 0.5\n",
    "scores = np.matmul(query, key.transpose(0,2,1)) / np.sqrt(query.shape[-1])\n",
    "print(mask)\n",
    "if mask is not None:\n",
    "    scores = np.where(mask == 0, -1e9, scores)\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  7.85563972e+00],\n",
       "        [ 1.76643157e+01,  1.46192234e+01,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  2.55480712e+00,  0.00000000e+00, ...,\n",
       "          6.89829850e+00,  0.00000000e+00,  1.26354108e+01],\n",
       "        ...,\n",
       "        [ 9.66596886e-15, -1.07562426e-14,  3.13325998e-15, ...,\n",
       "          1.81881504e-16, -1.67811657e-24,  8.07759595e-17],\n",
       "        [-1.57444034e-24, -5.52962824e-14,  7.06011442e-20, ...,\n",
       "          1.99687779e-14,  1.85037209e-28,  1.90711330e-14],\n",
       "        [ 1.75828430e-19, -7.85220312e-17, -2.43488462e-14, ...,\n",
       "         -1.08633817e-15,  3.42086588e-16, -4.00137991e-16]]),\n",
       " array([[ 0.72033332,  0.53810897,  2.00715621, ..., -0.1953115 ,\n",
       "          0.92959405,  0.02986244],\n",
       "        [-0.23327125, -0.80713075, -0.24969228, ...,  0.62572568,\n",
       "         -0.11806169, -0.75471715],\n",
       "        [-0.54308314, -0.73184676,  0.49433174, ..., -0.14840302,\n",
       "         -0.48541521, -0.22715831],\n",
       "        ...,\n",
       "        [ 0.95504941,  1.00791532,  0.5259679 , ...,  0.75033065,\n",
       "         -0.08138907, -0.22254324],\n",
       "        [ 0.17770502, -1.10751051, -0.80166373, ...,  1.22419417,\n",
       "          0.67085823,  1.0844837 ],\n",
       "        [ 0.75143884, -2.02849518, -0.69598314, ...,  0.04573965,\n",
       "         -0.43047958, -0.18534862]]),\n",
       " array([[ 0.41905305, -0.48067576, -0.30709098, ..., -0.73887657,\n",
       "          0.16898053, -0.07195191],\n",
       "        [ 0.84330015,  0.52764065, -0.1698496 , ..., -0.58696521,\n",
       "         -0.92570427, -0.6376576 ],\n",
       "        [-0.55329873, -1.02297462,  0.15384278, ...,  1.21266613,\n",
       "          1.60900968,  0.16511227],\n",
       "        ...,\n",
       "        [-0.19855814,  0.12296816, -0.61254266, ...,  0.27395295,\n",
       "          0.0140049 ,  1.44598942],\n",
       "        [-0.30538699,  0.9566753 ,  0.01369151, ..., -0.41399765,\n",
       "          0.37922165, -1.15308576],\n",
       "        [-1.30885892,  0.58373994,  0.34855431, ...,  0.0083538 ,\n",
       "          1.66568814,  0.21147701]])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layers = [np.empty((d_model, d_model)) for _ in range(3)]\n",
    "output_linear = np.empty((d_model, d_model))\n",
    "linear_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 512)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x, dim=None):\n",
    "    # x - np.max(x, axis=dim, keepdims=True)는 지수 함수의 지수가 너무 크거나 작아지는 것을 방지하기 위함\n",
    "    # keepdims=True는 차원을 유지하겠다는 의미\n",
    "    e_x = np.exp(x - np.max(x, axis=dim, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=dim, keepdims=True)\n",
    "\n",
    "class Attention:\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        # query, key, value shape: (batch_size, seq_len, d_k)\n",
    "        # mask shape: (batch_size, seq_len, seq_len)\n",
    "        scores = np.matmul(query, key.transpose(0,1,3,2)) / np.sqrt(query.shape[-1])\n",
    "        # make가 0인 부분은 -1e9로 채워준다.\n",
    "        if mask is not None:\n",
    "            scores = np.where(mask == 0, -1e9, scores)\n",
    "        # softmax 적용\n",
    "        p_attn = softmax(scores, dim=-1)\n",
    "        \n",
    "        if dropout is not None:\n",
    "            p_attn = dropout.forward(p_attn)\n",
    "            \n",
    "        return np.matmul(p_attn, value), p_attn\n",
    "    \n",
    "class MultiHeadedAttention:\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        assert d_model % h == 0\n",
    "        \n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        \n",
    "        self.linear_layers = [np.random.randn(d_model, d_model) for _ in range(3)]\n",
    "        self.output_linear = np.random.randn(d_model, d_model)\n",
    "        self.attention = Attention()\n",
    "        \n",
    "        self.dropout = Dropout(dropout)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # query, key, value: (batch_size, seq_len, d_embed)\n",
    "        # mask: (batch_size, seq_len, seq_len)\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        # query, key, value: (batch_size, seq_len, h, d_k) -> (batch_size, h, seq_len, d_k)\n",
    "        query, key, value = [np.matmul(x, l).reshape(batch_size, -1, self.h, self.d_k).transpose(0, 2, 1, 3) \\\n",
    "                                for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        # x: (batch_size, h, seq_len, d_k)\n",
    "        x, attn = self.attention.forward(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        # x: (batch_size, seq_len, h, d_k) -> (batch_size, seq_len, h*d_k = d_model)\n",
    "        x = x.transpose(0, 2, 1, 3).reshape(batch_size, -1, self.h * self.d_k)\n",
    "        print(x.shape)\n",
    "        return np.matmul(x, self.output_linear)\n",
    "\n",
    "x = MultiHeadedAttention(8, 512, 0).forward(np.random.rand(1, 2, 512), np.random.rand(1, 2, 512), np.random.rand(1, 2, 512)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 512)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x, train_flag=True):\n",
    "        # 학습 시\n",
    "        if train_flag:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_rate\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_rate)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "    \n",
    "class LayerNormalization:\n",
    "    \n",
    "    def __init__(self, eps: float = 10**-6): # We define epsilon as 0.000001 to avoid division by zero\n",
    "        self.eps = eps\n",
    "        \n",
    "        # 학습은 구현 안함\n",
    "        self.alpha = np.ones((1,)) \n",
    "        self.bias = np.zeros((1,))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(axis = -1, keepdims = True) # Computing the mean of the input data. Keeping the number of dimensions unchanged\n",
    "        std = x.std(axis = -1, keepdims = True) # Computing the standard deviation of the input data. Keeping the number of dimensions unchanged\n",
    "        \n",
    "        # Returning the normalized input\n",
    "        return self.alpha * (x-mean) / (std + self.eps) + self.bias\n",
    "\n",
    "import torch.nn as nn\n",
    "from .layer_norm import LayerNormalization\n",
    "from .dropout import Dropout\n",
    "\n",
    "class ResidualConnection:\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        self.norm = LayerNormalization(size)\n",
    "        self.dropout = Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout.forward(sublayer(self.norm(x)))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('seminar')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e199f158be38318a96f5c97bc7698436a0230787fc09064e8bff4e3316d7b79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
