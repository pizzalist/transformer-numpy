{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "pe = torch.zeros(3, 4)\n",
    "print(pe)\n",
    "print(pe[:, 0::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3,5)\n",
    "a.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.],\n",
       "        [5.],\n",
       "        [6.],\n",
       "        [7.],\n",
       "        [8.],\n",
       "        [9.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 10, dtype = torch.float).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.210340371976184"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "-math.log(10000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len, d_model = 5, 6\n",
    "pe = torch.zeros(seq_len, d_model) \n",
    "\n",
    "# Creating a tensor representing positions (0 to seq_len - 1)\n",
    "position = torch.arange(0, seq_len, dtype = torch.float).unsqueeze(1) # Transforming 'position' into a 2D tensor['seq_len, 1']\n",
    "\n",
    "# Creating the division term for the positional encoding formula\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "# Apply sine to even indices in pe\n",
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "# Apply cosine to odd indices in pe\n",
    "pe[:, 1::2] = torch.cos(position * div_term)\n",
    "pe = pe.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, seq_len, d_model =2, 5, 6\n",
    "pe = torch.zeros(batch_size, seq_len, d_model) \n",
    "pe.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 512])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from embedding.pt_position import PositionalEncoding\n",
    "\n",
    "pos_encoding = PositionalEncoding(d_model=512, seq_len=10, dropout=0.1)\n",
    "print(pos_encoding.pe.shape)\n",
    "x = torch.rand(5, 10, 512)\n",
    "output = pos_encoding(x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, 512)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "d_model = 512\n",
    "seq_len = 10\n",
    "\n",
    "position = np.arange(seq_len, dtype=float)[:, np.newaxis] \n",
    "div_term = np.exp(np.arange(0, d_model, 2, dtype=float)) #* -(np.log(10000.0) / d_model))\n",
    "pe = np.zeros((seq_len, d_model))\n",
    "pe[:, 0::2] = np.sin(position * div_term)\n",
    "# Apply cosine to odd indices in pe\n",
    "pe[:, 1::2] = np.cos(position * div_term)\n",
    "pe = np.expand_dims(pe, axis=0)\n",
    "pe.shape\n",
    "# x = np.random.rand(5, 10, 512)\n",
    "\n",
    "pe[:, :x.shape[1]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 2 3]\n",
      "  [4 5 6]]]\n",
      "Shape of result1: (1, 2, 3)\n",
      "[[[1 2 3]\n",
      "  [4 5 6]]]\n",
      "Shape of result2: (1, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# pe 배열: (1, 4, 3) 형태의 3차원 배열\n",
    "pe = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]])\n",
    "\n",
    "# x 배열: (2, 2, 3) 형태의 3차원 배열\n",
    "x = np.random.rand(2, 2, 3)\n",
    "\n",
    "# pe[:, :x.shape[1]] 사용\n",
    "result1 = pe[:, :x.shape[1]]\n",
    "print(result1)\n",
    "print(\"Shape of result1:\", result1.shape)\n",
    "\n",
    "# pe[:, :x.shape[1], :] 사용\n",
    "result2 = pe[:, :x.shape[1], :]\n",
    "print(result2)\n",
    "print(\"Shape of result2:\", result2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.8974e-02, -3.7333e-01, -1.2893e+00,  ..., -1.0914e+00,\n",
       "          -1.4434e+00, -4.5408e-01],\n",
       "         [-9.6912e-01,  1.4086e+00, -3.6165e-01,  ...,  7.4841e-02,\n",
       "          -4.1845e-01,  4.6434e-04],\n",
       "         [ 1.8011e+00,  1.6231e+00,  9.6974e-01,  ...,  2.4877e-01,\n",
       "           8.8367e-01,  1.0459e-01],\n",
       "         ...,\n",
       "         [ 1.5495e+00, -2.4305e-01,  1.1839e+00,  ..., -7.5035e-01,\n",
       "          -1.0053e+00,  6.2943e-01],\n",
       "         [ 5.8528e-01,  1.1627e-01, -4.7579e-01,  ..., -7.7362e-01,\n",
       "           2.3335e-01, -3.1856e-02],\n",
       "         [ 1.0478e+00,  1.6069e+00, -1.4900e+00,  ..., -3.6156e-01,\n",
       "          -4.6621e-01,  1.1313e+00]],\n",
       "\n",
       "        [[ 1.4661e+00, -9.2386e-02,  1.3521e+00,  ...,  3.2525e-01,\n",
       "          -6.6381e-01, -9.4885e-01],\n",
       "         [ 1.4223e+00, -1.4152e+00, -1.3470e+00,  ...,  4.6218e-01,\n",
       "           1.2839e+00, -1.2251e+00],\n",
       "         [ 4.8338e-01,  1.5696e+00, -7.2131e-01,  ..., -6.4744e-01,\n",
       "           8.2422e-03, -6.0463e-01],\n",
       "         ...,\n",
       "         [-1.6825e+00,  1.2184e+00,  8.4737e-02,  ...,  1.1206e+00,\n",
       "           1.5251e+00, -8.6183e-01],\n",
       "         [ 1.2871e+00, -1.1304e+00, -1.3292e+00,  ..., -4.1422e-01,\n",
       "          -1.4452e+00,  1.7855e-02],\n",
       "         [-2.7602e-01,  1.6851e+00, -3.3825e-01,  ..., -6.3957e-01,\n",
       "           1.0654e+00, -1.4320e+00]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "eps = 10**-6\n",
    "x = torch.tensor(np.random.rand(2, 10, 512), dtype=torch.float32)\n",
    "alpha = nn.Parameter(torch.ones(1)) # One-dimensional tensor that will be used to scale the input data\n",
    "\n",
    "bias = nn.Parameter(torch.zeros(1)) # One-dimensional tenso that will be added to the input data\n",
    "alpha, bias\n",
    "\n",
    "mean = x.mean(dim = -1, keepdim = True) # Computing the mean of the input data. Keeping the number of dimensions unchanged\n",
    "std = x.std(dim = -1, keepdim = True) # Computing the standard deviation of the input data. Keeping the number of dimensions unchanged\n",
    "        \n",
    "alpha * (x-mean) / (std + eps) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.406948  ,  1.25346707, -0.42135117, ..., -0.97828444,\n",
       "          1.6543173 , -0.46640261],\n",
       "        [ 0.59150044,  0.99039293, -0.40004363, ...,  0.21679665,\n",
       "         -0.37536094,  0.17018866],\n",
       "        [-0.90687934,  0.31464981, -0.90301743, ...,  0.65993653,\n",
       "         -1.31050138,  0.54569101],\n",
       "        ...,\n",
       "        [-1.21250908, -0.27937219, -0.00859462, ..., -0.19181777,\n",
       "          1.17208064,  1.01407876],\n",
       "        [-0.01594736, -0.2376258 ,  1.3790151 , ...,  1.01714823,\n",
       "          0.70535331,  0.12603634],\n",
       "        [-0.29881791, -1.56283988, -0.63784847, ...,  1.31632397,\n",
       "         -1.31980812,  1.00559515]],\n",
       "\n",
       "       [[-1.49989245,  1.02475926, -0.1921105 , ..., -0.04745001,\n",
       "         -1.34603429, -0.78242785],\n",
       "        [-0.70973579,  1.41222795, -0.24558219, ...,  0.42040351,\n",
       "          0.5467505 , -0.28627766],\n",
       "        [ 0.91151198, -0.03843883, -0.51292036, ..., -1.30789123,\n",
       "         -0.96578559,  0.10643825],\n",
       "        ...,\n",
       "        [ 1.590901  , -0.60996341, -0.20250717, ...,  0.26296964,\n",
       "          1.7038506 , -0.81559752],\n",
       "        [ 0.45552191,  1.51030739, -1.27669041, ..., -0.14840544,\n",
       "          0.62068237, -0.89572402],\n",
       "        [-1.55494965,  0.17569387,  0.45620587, ..., -1.56184161,\n",
       "          1.46038497,  0.94949201]]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "eps = 10**-6\n",
    "alpha = np.ones((1,)) \n",
    "bias = np.zeros((1,))\n",
    "x = np.random.rand(2, 10, 512)\n",
    "mean = x.mean(axis = -1, keepdims=True) # Computing the mean of the input data. Keeping the number of dimensions unchanged\n",
    "std = x.std(axis = -1, keepdims=True) # Computing the standard deviation of the input data. Keeping the number of dimensions unchanged\n",
    "        \n",
    "alpha * (x-mean) / (std + eps) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed forward test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 10, 512)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x, train_flag=True):\n",
    "        # 학습 시\n",
    "        if train_flag:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_rate\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_rate)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "\n",
    "class ReLU:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, x, grad_output):\n",
    "        relu_grad = x > 0\n",
    "        return grad_output * relu_grad\n",
    "\n",
    "class FeedForwardBlock:\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        # forward만 구현, random한 값으로 임의 init\n",
    "        self.W1 = np.random.randn(d_model, d_ff) \n",
    "        self.b1 = np.random.randn(d_ff)\n",
    "        self.W2 = np.random.randn(d_ff, d_model)\n",
    "        self.b2 = np.random.randn(d_model)\n",
    "        \n",
    "        self.actiavation = ReLU()\n",
    "        self.dropout = Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # (Batch, seq_len, d_model) --> (batch, seq_len, d_ff)\n",
    "        linear_1 = self.dropout.forward(self.actiavation.forward(np.dot(x, self.W1) + self.b1))\n",
    "        \n",
    "        # (batch, seq_len, d_ff) -->(batch, seq_len, d_model)\n",
    "        linear_2 = np.dot(linear_1, self.W2) + self.b2\n",
    "        return linear_2\n",
    "\n",
    "FeedForwardBlock(512, 2048, 0.1).forward(np.random.rand(2, 10, 512)).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('seminar')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e199f158be38318a96f5c97bc7698436a0230787fc09064e8bff4e3316d7b79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
