{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "pe = torch.zeros(3, 4)\n",
    "print(pe)\n",
    "print(pe[:, 0::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3,5)\n",
    "a.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.],\n",
       "        [5.],\n",
       "        [6.],\n",
       "        [7.],\n",
       "        [8.],\n",
       "        [9.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 10, dtype = torch.float).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.210340371976184"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "-math.log(10000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len, d_model = 5, 6\n",
    "pe = torch.zeros(seq_len, d_model) \n",
    "\n",
    "# Creating a tensor representing positions (0 to seq_len - 1)\n",
    "position = torch.arange(0, seq_len, dtype = torch.float).unsqueeze(1) # Transforming 'position' into a 2D tensor['seq_len, 1']\n",
    "\n",
    "# Creating the division term for the positional encoding formula\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "# Apply sine to even indices in pe\n",
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "# Apply cosine to odd indices in pe\n",
    "pe[:, 1::2] = torch.cos(position * div_term)\n",
    "pe = pe.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, seq_len, d_model =2, 5, 6\n",
    "pe = torch.zeros(batch_size, seq_len, d_model) \n",
    "pe.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 512])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from embedding.pt_position import PositionalEncoding\n",
    "\n",
    "pos_encoding = PositionalEncoding(d_model=512, seq_len=10, dropout=0.1)\n",
    "print(pos_encoding.pe.shape)\n",
    "x = torch.rand(5, 10, 512)\n",
    "output = pos_encoding(x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, 512)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "d_model = 512\n",
    "seq_len = 10\n",
    "\n",
    "position = np.arange(seq_len, dtype=float)[:, np.newaxis] \n",
    "div_term = np.exp(np.arange(0, d_model, 2, dtype=float)) #* -(np.log(10000.0) / d_model))\n",
    "pe = np.zeros((seq_len, d_model))\n",
    "pe[:, 0::2] = np.sin(position * div_term)\n",
    "# Apply cosine to odd indices in pe\n",
    "pe[:, 1::2] = np.cos(position * div_term)\n",
    "pe = np.expand_dims(pe, axis=0)\n",
    "pe.shape\n",
    "# x = np.random.rand(5, 10, 512)\n",
    "\n",
    "pe[:, :x.shape[1]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 2 3]\n",
      "  [4 5 6]]]\n",
      "Shape of result1: (1, 2, 3)\n",
      "[[[1 2 3]\n",
      "  [4 5 6]]]\n",
      "Shape of result2: (1, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# pe 배열: (1, 4, 3) 형태의 3차원 배열\n",
    "pe = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]])\n",
    "\n",
    "# x 배열: (2, 2, 3) 형태의 3차원 배열\n",
    "x = np.random.rand(2, 2, 3)\n",
    "\n",
    "# pe[:, :x.shape[1]] 사용\n",
    "result1 = pe[:, :x.shape[1]]\n",
    "print(result1)\n",
    "print(\"Shape of result1:\", result1.shape)\n",
    "\n",
    "# pe[:, :x.shape[1], :] 사용\n",
    "result2 = pe[:, :x.shape[1], :]\n",
    "print(result2)\n",
    "print(\"Shape of result2:\", result2.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('seminar')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e199f158be38318a96f5c97bc7698436a0230787fc09064e8bff4e3316d7b79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
